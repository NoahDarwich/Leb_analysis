{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "from google.oauth2.credentials import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "\n",
    "import json\n",
    "# from pdfminer.converter import TextConverter\n",
    "# from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "# from pdfminer.pdfinterp import PDFResourceManager\n",
    "# from pdfminer.pdfpage import PDFPage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\n",
    "    url='https://proxy.scrapeops.io/v1/',\n",
    "    params={\n",
    "        'api_key': '378d2b00-d774-4869-bf97-325ac3d1de22',\n",
    "        'url': 'https://www.al-akhbar.com/Editions',\n",
    "    },\n",
    "    )\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = soup.find('div', {'class': 'l-content row archive-issues'})\n",
    "container = container.find('div', id='archive-days-wrap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "\n",
    "for a_tag in container.find_all('a', class_='day'):\n",
    "    data_id = a_tag.get('data-id')\n",
    "    link = a_tag.get('href')\n",
    "    url = 'https://www.al-akhbar.com' + link + '/PDF'\n",
    "    data_name = a_tag.get('data-oldtitle')\n",
    "    date = link[10:].replace('/','-')\n",
    "    dic[data_id] = [data_name,url,date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=dic).T\n",
    "df.columns = ['issue', 'url', 'date']\n",
    "df = df[(df['date'] > '2010-01-01') & (df['date'] < '2020-01-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_pdf(url):\n",
    "    \n",
    "    retries = Retry(total=5,\n",
    "                status_forcelist=[429, 500, 502, 503, 504])\n",
    "    response = requests.get(\n",
    "    url='https://proxy.scrapeops.io/v1/',\n",
    "    params={\n",
    "        'api_key': '378d2b00-d774-4869-bf97-325ac3d1de22',\n",
    "        'url': url,\n",
    "    },\n",
    "    )\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    try:\n",
    "        container = soup.find('div', {'class': 'content'})\n",
    "        pdf_link = container.find('a').get('href')\n",
    "        pdf_link = 'https://www.al-akhbar.com' + pdf_link\n",
    "        print('tamam')\n",
    "        return pdf_link\n",
    "    \n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.pdf_link.isna())\n",
    "df2 = df.loc[mask]\n",
    "df2['pdf_link'] = df2['url'].apply(get_pdf)\n",
    "df2.to_csv('new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask3 = (df2.pdf_link.isna())\n",
    "df3 = df2.loc[mask3]\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['pdf_link'] = df3['url'].apply(get_pdf)\n",
    "df3.to_csv('new2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('new3.csv')\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['pdf_link'] = df3['url'].apply(get_pdf)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('pdf_df.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('new.csv')\n",
    "df2 = pd.read_csv('new2.csv')\n",
    "df3 = pd.read_csv('new3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pdf_link.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([df1,df2,df3], ignore_index=True)\n",
    "full_df.drop(columns=['Unnamed: 0'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.pdf_link.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['pdf_link'].isna(), 'pdf_link'] = full_df['pdf_link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['pdf_link'].isna(), 'pdf_link'] = df['url'].apply(get_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pdf_link.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['pdf_link'].isna(), 'pdf_link'] = df.loc[df['pdf_link'].isna(), 'url'].apply(get_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Full_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Full_df.csv')\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/noahdarwich/code/NoahDarwich/Leb_analysis/Notebooks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pdfs(df):\n",
    "    \n",
    "    unsaved = []\n",
    "    df = df.dropna()\n",
    "    for i, j in df.iterrows():\n",
    "        \n",
    "        name = j['date']+'.pdf'\n",
    "        url = j['pdf_link']\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        \n",
    "        try:\n",
    "            os.chdir('/mnt/d')\n",
    "            with open(name, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "                print('tamam')\n",
    "        except:\n",
    "            unsaved.append(name)\n",
    "            print('No')\n",
    "    return unsaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsaved = save_pdfs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile \n",
    "from oauth2client.service_account import ServiceAccountCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = df.date[0]+'.pdf'\n",
    "link = df.pdf_link[0]\n",
    "response = requests.get(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = ServiceAccountCredentials.from_json_keyfile_name('leba-375419-61adff20f07d.json', scopes=['https://www.googleapis.com/auth/drive'])\n",
    "\n",
    "service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "#file_metadata = {'name': name}\n",
    "\n",
    "\n",
    "\n",
    "#file = service.files().create(body=file_metadata, media_body=media,\n",
    "#                                    fields='id').execute()\n",
    "#print(F'File ID: {file.get(\"id\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_metadata = {\n",
    "    'name': 'Al_Akhbar_2010_2019',\n",
    "    'mimeType': 'application/vnd.google-apps.folder'\n",
    "}\n",
    "folder = service.files().create(body=folder_metadata, fields='id').execute()\n",
    "folder_id = folder.get('id')\n",
    "folder_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = tempfile.NamedTemporaryFile()\n",
    "file.write(response.content)\n",
    "file.seek(0)\n",
    "\n",
    "file_metadata = {'name': name,'parents':['1Le84wtKcUo5wP3bcBluZRmMcZnFsBNrk']}\n",
    "media = MediaFileUpload(file.name,\n",
    "                        mimetype='application/pdf')\n",
    "\n",
    "file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_article_to_drive(article_text, file_name):\n",
    "#     try:\n",
    "#       # authenticate with Google Drive\n",
    "#       credentials = Credentials.from_authorized_user_info()\n",
    "#       service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "#       # create the file on Google Drive\n",
    "#       file_metadata = {'name': file_name, 'mimeType': 'text/plain'}\n",
    "#       media = MediaIoBaseUpload(io.StringIO(article_text), mimetype='text/plain',chunksize=1024*1024, resumable=True)\n",
    "#       file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "#       print(f'File ID: {file.get(/id\\)}')\n",
    "      \n",
    "#     except HttpError as error:\n",
    "#         print(f'An error occurred: {error}')\n",
    "#         file = None\n",
    "#     return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the website for PDF links\n",
    "\n",
    "# download each PDF and save it to Google Drive\n",
    "\n",
    "text_ls = [] \n",
    "for link in pdf_links:\n",
    "    # download the PDF\n",
    "    pdf_url = link['href']\n",
    "    \n",
    "    pdf_content = pdf_response.content\n",
    "    # extract the text from the PDF\n",
    "    text = extract_text_from_pdf(pdf_content)\n",
    "    text_ls.append(text)\n",
    "    \n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_text_from_pdf(pdf_content):\n",
    "#   resource_manager = PDFResourceManager()\n",
    "#   fake_file_handle = io.StringIO()\n",
    "#   converter = TextConverter(resource_manager, fake_file_handle)\n",
    "#   page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "\n",
    "#   with io.BytesIO(pdf_content) as fh:\n",
    "#     for page in PDFPage.get_pages(fh,\n",
    "#                                   caching=True,\n",
    "#                                   check_extractable=True):\n",
    "#       page_interpreter.process_page(page)\n",
    "\n",
    "#     text = fake_file_handle.getvalue()\n",
    "\n",
    "#   # close open handles\n",
    "#   converter.close()\n",
    "#   fake_file_handle.close()\n",
    "\n",
    "#   return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"# search the text for keywords\n",
    "# \"keywords = ['keyword1', 'keyword2', 'keyword3']\n",
    "# \"for keyword in keywords:\n",
    "# \"  if re.search(keyword, text, re.IGNORECASE):\n",
    "# \"    # keyword found, save the article\n",
    "# \"    file_name = f'{keyword}_article.txt'\n",
    "# \"    save_article_to_drive(text, file_name)\n",
    "# \"    break\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08f894913cde66c7b668fed923c75d3a020a4a5d16d7076f7658aed24e50b1e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
